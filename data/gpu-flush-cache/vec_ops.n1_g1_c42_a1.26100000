[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------
[0]PETSC ERROR: Error in external library
[0]PETSC ERROR: CUDA error 2
[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.
[0]PETSC ERROR: Petsc Release Version 3.12.0, unknown 
[0]PETSC ERROR: /ccs/home/hmorgan/petsc-v3.12/src/vec/vec/examples/tests/hannah-vec-ops on a arch-olcf-summit-opt named a09n18 by hmorgan Fri Jan 10 12:22:16 2020
[0]PETSC ERROR: Configure options --with-cc=mpicc --with-cxx=mpiCC --with-fc=mpifort --with-shared-libraries=1 --with-debugging=no COPTFLAGS="-g -fast" CXXOPTFLAGS="-g -fast" FOPTFLAGS="-g -fast" --download-fblaslapack=1 --with-cuda=1 --with-cudac=nvcc CUDAFLAGS="-ccbin pgc++" --download-viennacl=1 --with-hdf5-dir=/autofs/nccs-svm1_sw/summit/.swci/1-compute/opt/spack/20180914/linux-rhel7-ppc64le/pgi-19.4/hdf5-1.10.3-pgiul2yf4auv7krecd72t6vupd7e3qgn --download-metis=1 --download-parmetis=1 --download-triangle=1 --download-ctetgen=1 --download-ml=1 PETSC_ARCH=arch-olcf-summit-opt
[0]PETSC ERROR: #1 VecCUDAAllocateCheck() line 34 in /autofs/nccs-svm1_home1/hmorgan/petsc-v3.12/src/vec/vec/impls/seq/seqcuda/veccuda2.cu
[0]PETSC ERROR: #2 VecCreate_MPICUDA_Private() line 341 in /autofs/nccs-svm1_home1/hmorgan/petsc-v3.12/src/vec/vec/impls/mpi/mpicuda/mpicuda.cu
[0]PETSC ERROR: #3 VecDuplicate_MPICUDA() line 127 in /autofs/nccs-svm1_home1/hmorgan/petsc-v3.12/src/vec/vec/impls/mpi/mpicuda/mpicuda.cu
[0]PETSC ERROR: #4 VecDuplicate() line 365 in /autofs/nccs-svm1_home1/hmorgan/petsc-v3.12/src/vec/vec/interface/vector.c
[0]PETSC ERROR: #5 main() line 30 in /ccs/home/hmorgan/petsc-v3.12/src/vec/vec/examples/tests/hannah-vec-ops.c
[0]PETSC ERROR: PETSc Option Table entries:
[0]PETSC ERROR: -log_view
[0]PETSC ERROR: -n 26100000
[0]PETSC ERROR: -vec_type mpicuda
[0]PETSC ERROR: ----------------End of Error Message -------send entire error message to petsc-maint@mcs.anl.gov----------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 76.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
